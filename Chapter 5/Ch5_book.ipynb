{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 5 - Actor-Critic Models\n",
    "### Deep Reinforcement Learning in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "# jupyter에선 이런 방식의 multiprocessing이 작동하지 않음\n",
    "# 따라서 worker.py 파일을 추가하여 원하는 함수를 따로 지정 \n",
    "from worker import square\n",
    "# def square(x): #A\n",
    "#     print(\"x:\",x)\n",
    "#     return np.square(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_cpu = mp.cpu_count()\n",
    "x = np.arange(num_cpu * num_cpu) #B\n",
    "print(x)\n",
    "print(mp.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pool = mp.Pool(8) #C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  0,   1,   4,   9,  16,  25,  36,  49,  64,  81, 100, 121]), array([144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529]), array([ 576,  625,  676,  729,  784,  841,  900,  961, 1024, 1089, 1156,\n",
      "       1225]), array([1296, 1369, 1444, 1521, 1600, 1681, 1764, 1849, 1936, 2025, 2116,\n",
      "       2209]), array([2304, 2401, 2500, 2601, 2704, 2809, 2916, 3025, 3136, 3249, 3364,\n",
      "       3481]), array([3600, 3721, 3844, 3969, 4096, 4225, 4356, 4489, 4624, 4761, 4900,\n",
      "       5041]), array([5184, 5329, 5476, 5625, 5776, 5929, 6084, 6241, 6400, 6561, 6724,\n",
      "       6889]), array([7056, 7225, 7396, 7569, 7744, 7921, 8100, 8281, 8464, 8649, 8836,\n",
      "       9025]), array([ 9216,  9409,  9604,  9801, 10000, 10201, 10404, 10609, 10816,\n",
      "       11025, 11236, 11449]), array([11664, 11881, 12100, 12321, 12544, 12769, 12996, 13225, 13456,\n",
      "       13689, 13924, 14161]), array([14400, 14641, 14884, 15129, 15376, 15625, 15876, 16129, 16384,\n",
      "       16641, 16900, 17161]), array([17424, 17689, 17956, 18225, 18496, 18769, 19044, 19321, 19600,\n",
      "       19881, 20164, 20449])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "squared = pool.map(square, [x[num_cpu*i:num_cpu*i+num_cpu] for i in range(num_cpu)])\n",
    "print(squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from worker import square2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 64,  81, 100, 121, 144, 169, 196, 225]), array([ 0,  1,  4,  9, 16, 25, 36, 49]), array([1024, 1089, 1156, 1225, 1296, 1369, 1444, 1521]), array([1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209]), array([2304, 2401, 2500, 2601, 2704, 2809, 2916, 3025]), array([256, 289, 324, 361, 400, 441, 484, 529]), array([576, 625, 676, 729, 784, 841, 900, 961]), array([3136, 3249, 3364, 3481, 3600, 3721, 3844, 3969])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def square2(i, x, queue):\n",
    "#     print(\"In process {}\".format(i,))\n",
    "#     queue.put(np.square(x))\n",
    "\n",
    "# 프로세스를 더 명시적으로 제어하고 공유 가능한 자료구조를 이용해서 프로세서들이 데이터 공유 가능 \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    processes = [] #A  프로세스의 참조(포인터같은 느낌인가?)를 담을 리스트\n",
    "    queue = mp.Queue() #B\n",
    "    x = np.arange(64) #C\n",
    "    for i in range(8): #D\n",
    "        start_index = 8*i\n",
    "        \n",
    "        proc = mp.Process(target=square2,args=(i,x[start_index:start_index+8], queue)) \n",
    "        proc.start()\n",
    "        processes.append(proc)\n",
    "        \n",
    "    for proc in processes: #E 모든 프로세스가 완료되길 기다림 \n",
    "        proc.join()\n",
    "        \n",
    "    for proc in processes: #F 완료되었다면 모든 프로세스 종료 \n",
    "        proc.terminate()\n",
    "\n",
    "    results = []\n",
    "    while not queue.empty(): #G queue에서 순서대로 결과를 담아옴 \n",
    "        results.append(queue.get())\n",
    "\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import gym\n",
    "import torch.multiprocessing as mp #A\n",
    "\n",
    "class ActorCritic(nn.Module): #B\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.l1 = nn.Linear(4,25)\n",
    "        self.l2 = nn.Linear(25,50)\n",
    "        self.actor_lin1 = nn.Linear(50,2)\n",
    "        self.l3 = nn.Linear(50,25)\n",
    "        self.critic_lin1 = nn.Linear(25,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.normalize(x,dim=0)\n",
    "        y = F.relu(self.l1(x))\n",
    "        y = F.relu(self.l2(y))\n",
    "        # l2를 통과한 결과로 actor와 critic을 모두 사용한다 \n",
    "        # log_softmax(x) := log(softmax(x)) 와 같지만 따로 사용하는 것보다 안정성이 높음 \n",
    "        actor = F.log_softmax(self.actor_lin1(y),dim=0) #C\n",
    "        c = F.relu(self.l3(y.detach()))  # critic은 역전파되지 않음 \n",
    "        # critic은 -1~1 사이의 값을 출력한다 \n",
    "        # 이러한 값은 보상이 -1, +1인 cartpole과 잘 맞는다 \n",
    "        critic = torch.tanh(self.critic_lin1(c)) #D\n",
    "        return actor, critic #E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 5.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from worker import cworker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef worker(t, worker_model, counter, params):\\n    worker_env = gym.make(\"CartPole-v1\")\\n    worker_env.reset()\\n    # 하나의 모형을 모든 프로세스가 공유 \\n    worker_opt = optim.Adam(lr=1e-4,params=worker_model.parameters()) #A\\n    worker_opt.zero_grad()\\n    for i in range(params[\\'epochs\\']):\\n        worker_opt.zero_grad()\\n        # episode를 진행하여 데이터를 수집하고 매개변수들을 갱신한다 \\n        values, logprobs, rewards = run_episode(worker_env,worker_model) #B \\n        actor_loss,critic_loss,eplen = update_params(worker_opt,values,logprobs,rewards) #C\\n        counter.value = counter.value + 1 #D\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def worker(t, worker_model, counter, params):\n",
    "    worker_env = gym.make(\"CartPole-v1\")\n",
    "    worker_env.reset()\n",
    "    # 하나의 모형을 모든 프로세스가 공유 \n",
    "    worker_opt = optim.Adam(lr=1e-4,params=worker_model.parameters()) #A\n",
    "    worker_opt.zero_grad()\n",
    "    for i in range(params['epochs']):\n",
    "        worker_opt.zero_grad()\n",
    "        # episode를 진행하여 데이터를 수집하고 매개변수들을 갱신한다 \n",
    "        values, logprobs, rewards = run_episode(worker_env,worker_model) #B \n",
    "        actor_loss,critic_loss,eplen = update_params(worker_opt,values,logprobs,rewards) #C\n",
    "        counter.value = counter.value + 1 #D\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Listing 5.8\n",
    "def update_params(worker_opt,values,logprobs,rewards,clc=0.1,gamma=0.95):\n",
    "    # flip: 성분들을 모두 역순으로 정렬\n",
    "    # view(-1): 평평하게 만듦, 1차원 배열이 아닌 텐서를 넘겨줄 수도 있기 때문 \n",
    "    rewards = torch.Tensor(rewards).flip(dims=(0,)).view(-1) #A\n",
    "    logprobs = torch.stack(logprobs).flip(dims=(0,)).view(-1)\n",
    "    values = torch.stack(values).flip(dims=(0,)).view(-1)\n",
    "\n",
    "    Returns = []\n",
    "    ret_ = torch.Tensor([0])\n",
    "    # 역순으로 return을 계산해서 Returns 배열에 저장\n",
    "    for r in range(rewards.shape[0]): #B\n",
    "        ret_ = rewards[r] + gamma * ret_\n",
    "        Returns.append(ret_)\n",
    "    Returns = torch.stack(Returns).view(-1)\n",
    "    Returns = F.normalize(Returns,dim=0)\n",
    "    # actor의 손실 계산, critic의 loss가 역전파되지 않도록 detach 사용 \n",
    "    actor_loss = -1*logprobs * (Returns - values.detach()) #C\n",
    "    # critic의 손실은 작을 수록 보상을 더 잘 예측하게 함 \n",
    "    critic_loss = torch.pow(values - Returns,2) #D\n",
    "    # critic의 loss 비율은 적절한 비율로 감소시킴 \n",
    "    loss = actor_loss.sum() + clc*critic_loss.sum() #E\n",
    "    loss.backward()\n",
    "    worker_opt.step()\n",
    "    return actor_loss, critic_loss, len(rewards)\n",
    "\n",
    "##### Listing 5.7\n",
    "def run_episode(worker_env, worker_model):\n",
    "    state = torch.from_numpy(worker_env.env.state).float() #A\n",
    "    values, logprobs, rewards = [],[],[] #B state value, log probability, reward\n",
    "    done = False\n",
    "    j=0\n",
    "    while (done == False): #C\n",
    "        j+=1\n",
    "        policy, value = worker_model(state) #D\n",
    "        values.append(value)\n",
    "        logits = policy.view(-1)\n",
    "        action_dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = action_dist.sample() #E  확률에 따라 sampling\n",
    "        logprob_ = policy.view(-1)[action]\n",
    "        logprobs.append(logprob_)\n",
    "        # 뽑은 action에 따라 다음 step 진행 \n",
    "        state_, _, done, info = worker_env.step(action.detach().numpy())\n",
    "        state = torch.from_numpy(state_).float()\n",
    "        if done: #F\n",
    "            reward = -10\n",
    "            worker_env.reset()\n",
    "        else:\n",
    "            reward = 1.0\n",
    "        rewards.append(reward)\n",
    "    return values, logprobs, rewards\n",
    "\n",
    "\n",
    "# Listing 5.6\n",
    "def cworker(t, worker_model, counter, params):\n",
    "    # print(\"t is:\",t, flush=True)\n",
    "    sys.stdout = open(str(os.getpid()) + \".out\", \"w\")\n",
    "    info('function cworker')\n",
    "    print ('hello')\n",
    "    worker_env = gym.make(\"CartPole-v1\")\n",
    "    worker_env.reset()\n",
    "    # 하나의 모형을 모든 프로세스가 공유 \n",
    "    worker_opt = optim.Adam(lr=1e-4,params=worker_model.parameters()) #A\n",
    "    worker_opt.zero_grad()\n",
    "    for i in range(params['epochs']):\n",
    "        worker_opt.zero_grad()\n",
    "        # episode를 진행하여 데이터를 수집하고 매개변수들을 갱신한다 \n",
    "        values, logprobs, rewards = run_episode(worker_env,worker_model) #B \n",
    "        actor_loss,critic_loss,eplen = update_params(worker_opt,values,logprobs,rewards) #C\n",
    "        counter.value = counter.value + 1 #D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "MasterNode = ActorCritic()\n",
    "MasterNode.share_memory()  # 프로세스들이 model의 매개변수를 복사하는게 아니라 공유하게 함 \n",
    "processes = []\n",
    "params = {\n",
    "    'epochs':1000,\n",
    "    'n_workers':7,\n",
    "}\n",
    "# 내장 공유 객체를 전역 공유 카운터로 사용\n",
    "counter = mp.Value('i',0)  # i는 정수라는 뜻 \n",
    "if __name__ == '__main__': #adding this for process safety\n",
    "    for i in range(params['n_workers']):\n",
    "        p = mp.Process(target=cworker, args=(i,MasterNode,counter, params))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    print(len(processes))\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    for p in processes:\n",
    "        p.terminate()\n",
    "    # 전역 카운터의 값과 첫 프로세스의 종료코드 출력(문제 없으면 0)\n",
    "    print(counter.value,processes[1].exitcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost\n",
      "Lost\n",
      "Lost\n",
      "Lost\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "env.reset()\n",
    "frames = []\n",
    "screen = env.render()\n",
    "images = [Image.fromarray(screen)]\n",
    "for i in range(100):\n",
    "    \n",
    "    state_ = np.array(env.env.state)\n",
    "    state = torch.from_numpy(state_).float()\n",
    "    logits,value = MasterNode(state)\n",
    "    action_dist = torch.distributions.Categorical(logits=logits)\n",
    "    action = action_dist.sample()\n",
    "    # print(env.step(action.detach().numpy()))\n",
    "    state2, reward, done, _, info = env.step(action.detach().numpy())\n",
    "    if done:\n",
    "        print(\"Lost\")\n",
    "        env.reset()\n",
    "    state_ = np.array(env.env.state)\n",
    "    state = torch.from_numpy(state_).float()\n",
    "    screen = env.render()\n",
    "    images.append(Image.fromarray(screen))\n",
    "    #frames.append(env.render())\n",
    "    \n",
    "env.close()\n",
    "# images = render_episode(render_env, model, max_steps_per_episode)\n",
    "image_file = 'cartpole-v1.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(image_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# display_frames_as_gif(frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 5.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(worker_env, worker_model, N_steps=10):\n",
    "    raw_state = np.array(worker_env.env.state)\n",
    "    state = torch.from_numpy(raw_state).float()\n",
    "    values, logprobs, rewards = [],[],[]\n",
    "    done = False\n",
    "    j=0\n",
    "    G=torch.Tensor([0]) #A\n",
    "    while (j < N_steps and done == False): #B\n",
    "        j+=1\n",
    "        policy, value = worker_model(state)\n",
    "        values.append(value)\n",
    "        logits = policy.view(-1)\n",
    "        action_dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = action_dist.sample()\n",
    "        logprob_ = policy.view(-1)[action]\n",
    "        logprobs.append(logprob_)\n",
    "        state_, _, done, info = worker_env.step(action.detach().numpy())\n",
    "        state = torch.from_numpy(state_).float()\n",
    "        if done:\n",
    "            reward = -10\n",
    "            worker_env.reset()\n",
    "        else: #C\n",
    "            reward = 1.0\n",
    "            G = value.detach()\n",
    "        rewards.append(reward)\n",
    "    return values, logprobs, rewards, G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 5.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bootstrapping\n",
      "0.010000000000000009 1.99\n",
      "With bootstrapping\n",
      "0.9901 2.9701\n"
     ]
    }
   ],
   "source": [
    "# bootstrapping의 유무에 따른 총 수익 변화 \n",
    "#Simulated rewards for 3 steps\n",
    "r1 = [1,1,-1]\n",
    "r2 = [1,1,1]\n",
    "R1,R2 = 0.0,0.0\n",
    "#No bootstrapping\n",
    "for i in range(len(r1)-1,0,-1): \n",
    "    R1 = r1[i] + 0.99*R1\n",
    "for i in range(len(r2)-1,0,-1):\n",
    "    R2 = r2[i] + 0.99*R2\n",
    "print(\"No bootstrapping\")\n",
    "print(R1,R2)\n",
    "#With bootstrapping\n",
    "R1,R2 = 1.0,1.0\n",
    "for i in range(len(r1)-1,0,-1):\n",
    "    R1 = r1[i] + 0.99*R1\n",
    "for i in range(len(r2)-1,0,-1):\n",
    "    R2 = r2[i] + 0.r99*R2\n",
    "print(\"With bootstrapping\")\n",
    "print(R1,R2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd36931ac047644edc8e37929f1b958acd5f5bee9e13d64bf06ebefc74cd4bff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
